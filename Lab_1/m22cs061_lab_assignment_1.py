# -*- coding: utf-8 -*-
"""M22CS061_Lab_Assignment_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FIWjfTV74WerTORq2RZ1iPR-_LCC59y6
"""

from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split

info = load_iris()

x = info.data
y = info.target

np.random.rand(4,1)

plt.scatter(x[:,2], x[:,3], c = y, cmap=plt.cm.viridis)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 64)

"""# Question 1"""

class SinglePerceptron():
  def __init__(self):
    self.weight = np.random.rand(4, 1)
    self.bias = np.random.rand(1)

  def forward(self, x):
    return  (x.dot(self.weight)) + self.bias

  def loss(self, y_true, y_pred):
    return (1/len(y_true))*np.sum((y_true - y_pred)**2)

  def loss_prime(self, y_true, y_pred):
    return (-2/len(y_true))*np.sum((y_true - y_pred))

  def update_params(self, weight_prime, bias_prime, lr):
    self.weight = self.weight - (lr * weight_prime)
    self.bias = self.bias - (lr * bias_prime)

  def backward(self, x, y_true, y_pred, lr):
    weight_prime = (1/len(x)) * self.weight
    bias_prime = (1/len(x)) * np.sum(self.loss_prime(y_true, y_pred))
    return weight_prime, bias_prime

  def grad(self, x_train, y_train, lr, epoch):
    
    for i in range(epoch):
      y_pred = self.forward(x_train)

      weight_prime, bias_prime = self.backward(x_train, y_train, y_pred, lr)
      print(f"w: {weight_prime}, \nb: {bias_prime}")

      self.update_params(weight_prime, bias_prime, lr)

      loss = self.loss(y_train, y_pred)
      print(f"\n{i}th loss: {loss}\n")

      # loss_ls.append(loss)

perceptron = SinglePerceptron()

loss_ll = perceptron.grad(x_train, y_train, 0.1, 2)

"""# Question 2"""

class MLP():
  def __init__(self):
    # hidden layer 1
    self.weight_1 = np.random.rand(4,5)
    self.bias_1 = np.random.rand(5)

    # hidden layer 2
    self.weight_2 = np.random.rand(5,3)
    self.bias_2 = np.random.rand(3)

  def soft_max(self, val):
    # a = []
    # for i in val:
      # a.append(np.exp(val) / np.sum(np.exp(val), axis = 0))
    return (np.exp(val) / np.sum(np.exp(val), axis = 0))

  def soft_max_prime(self, action_fun2, y):
    return action_fun2 - y


  def forward(self, x):
    # use ReLU for first and Sigmoid for second
    self.activation_f1 = np.maximum(0, (x.dot(self.weight_1) + self.bias_1))
    z = self.activation_f1.dot(self.weight_2) + self.bias_2
    # print(type(z), "\n", z)
    self.activation_f2 = []
    for i in z:
      self.activation_f2.append(self.soft_max(i))
    self.activation_f2 = np.array(self.activation_f2)

    return self.activation_f2


  def cross_entropy_loss(self, y_pred, y):
    # sum_1 = np.sum(np.multiply(cat_y, np.log(out_prob)))
    # size = y_trn.shape[0]
    # return -(1./(size * 10000)) * sum_1
    a = []
    for i in range(y):
      a.append(-np.sum(y[i]*np.log2(y_pred[i])))
    
    a = np.array(a)
    return (-np.sum(y*np.log2(y_pred)))

  def cross_entropy_prime(self, y_pred):
    return -(1+np.log2(y_pred))

  def backward(self, x, y_pred, lr):
    prime_f2 = self.soft_max_prime(self.activation_f2.T, y_pred)
    # print("prime_f2 ", type(prime_f2), "\n", prime_f2)

    weights_2_prime = (1/len(y_pred)) * prime_f2.dot(self.activation_f1)
    # print("weights_2_prime ", type(weights_2_prime), "\n", weights_2_prime)

    bias_2_prime = (1/len(y_pred)) * np.sum(prime_f2)

    activation_f1_prime = self.weight_2.dot(prime_f2)

    f1 = (x.dot(self.weight_1) + self.bias_1)

    mat  = []
    
    # print(type(f1), "\n", f1)
    for i in f1:
      x = []
      for j in i:
        if j > 0:
          x.append(1)
        else:
          x.append(0)

      mat.append(x)

    mat = np.array(mat)

    prime_f1 = activation_f1_prime.dot(mat)
    # print(type(prime_f1), "\n", prime_f1)

    weight_1_prime = (1/len(y_pred)) * prime_f1.dot(x)

    bias_1_prime = (1/len(y_pred)) * np.sum(prime_f1)

    return weights_2_prime, bias_2_prime, weight_1_prime, bias_1_prime


  def update_params(self, weights_2_prime, bias_2_prime, weight_1_prime, bias_1_prime, lr):
    # print(type(weights_2_prime), "\n", weights_2_prime)
    self.weight_2 += -lr*weights_2_prime.T
    self.bias_2 += -lr*bias_2_prime
    self.weight_1 += -lr*weight_1_prime
    self.bias_1 += -lr*bias_1_prime

  def accuracy(self, y_pred, y_trn):
    sum_1 = np.sum(y_pred == y_trn)
    return sum_1/y_trn.size

  def grad(self, x, y, lr, epoch):
    for i in range(epoch):
      y_logits = self.forward(x)

      y_pred = []
      for i in y_logits:
        y_pred.append(np.argmax(i))

      y_pred = np.array(y_pred)
      # print(type(y_logits), "\n",y_logits)
      # print(type(y_pred), "\n",y_pred)
      # print("\n", type(y), "\n",y)
      # loss = self.cross_entropy_loss(y_pred, y)

      weights_2_prime, bias_2_prime, weight_1_prime, bias_1_prime = self.backward(x, y_pred, lr)

      self.update_params(weights_2_prime, bias_2_prime, weight_1_prime, bias_1_prime, lr)

      acc = self.accuracy(y_pred, y)
      # print(f"\n acc: {acc} Loss: {loss}")
      print(f"\n acc: {acc}")

mlp = MLP()
print(f"Params: {mlp.weight_2}, \n{mlp.bias_2}, \n{mlp.weight_1}, \n{mlp.bias_1}")
mlp.grad(x_train, y_train, 0.2, 15)
print(f"\nParams: {mlp.weight_2}, \n{mlp.bias_2}, \n{mlp.weight_1}, \n{mlp.bias_1}")

