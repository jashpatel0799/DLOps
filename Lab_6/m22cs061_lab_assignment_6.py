# -*- coding: utf-8 -*-
"""M22CS061_Lab_Assignment_6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10R2ByykLt3VRBGSFHYovo6yxAna_zmAc
"""

!pip install -q torch-tb-profiler

!pip install -q onnx
!pip install -q onnxruntime

"""## Import require libraries"""

import torch
from torch import nn

import torchvision
from torchvision import models, transforms
from torchvision import datasets
from torchvision.transforms import ToTensor
from torch.utils.data import DataLoader

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

from tqdm.auto import tqdm
import gc

!pip install -q torchmetrics
from torchmetrics.classification import MulticlassAccuracy

import torch.profiler
import torch.nn.functional as F
from time import perf_counter
from copy import deepcopy
import torch.onnx
import onnxruntime
from onnxruntime.quantization import quantize_dynamic, QuantType
import onnx
import os

from pathlib import Path
from google.colab import drive
drive.mount('/content/drive/')

"""## Device agnostic code"""

# device agnostic code
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

"""## Load CIFAR 100 dataset

# Transforms for normalization and augmentation
"""

train_transform = transforms.Compose([
                                        # transforms.Resize((32, 32)),
                                        transforms.RandomHorizontalFlip(),
                                        # transforms.RandomCrop(32, 4),
                                        transforms.ToTensor(),
                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                             std=[0.229, 0.224, 0.225]),
                                        transforms.RandomErasing()
                                    ])
test_transform = transforms.Compose([
                                        # transforms.Resize((32, 32)),
                                        transforms.ToTensor(),
                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                             std=[0.229, 0.224, 0.225]),
                                    ])

train_dataset = datasets.CIFAR100(root = './data', train = True, transform = train_transform, download = True)
test_dataset = datasets.CIFAR100(root = './data', train = False, transform = test_transform, download = True)

len(train_dataset), len(test_dataset)

"""## Create dataloader"""

BATCH_SIZE = 32

train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)

test_dataloader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = False, drop_last = True)

print(f"DataLoader: {train_dataloader}, {test_dataloader}")
print(f"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}")
print(f"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}")

"""## Visulize Data"""

class_names = train_dataset.classes
class_names, len(class_names)

torch.manual_seed(64)
plt.figure(figsize=(10,10))
row, col = 3, 3

for i in range(1, row*col+1):
  random_idx = torch.randint(0, len(train_dataset), size = [1]).item()
  image, label = train_dataset[random_idx]
  plt.subplot(row, col, i)
  # plt.imshow(image.squeeze(), cmap = 'gray')
  plt.imshow(image.permute(1,2,0))
  plt.title(class_names[label])
  plt.axis(False)

"""## Terain for tensorboard"""

def train(data, model: torch.nn.Module, loss_fn, optimizer):
    inputs, labels = data[0].to(device=device), data[1].to(device=device)
    outputs = model(inputs)
    loss = loss_fn(outputs, labels)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

"""## Train and Test Loop"""

# train
def train_loop(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer,
               accuracy_fn, device: torch.device = device):
  
  train_loss, train_acc = 0, 0

  start_time = perf_counter()
  for batch, (x_train, y_train) in enumerate(dataloader):

    if device == 'cuda':
      x_train, y_train = x_train.to(device), y_train.to(device)

    model.train()

    # 1. Forward step
    pred = model(x_train)

    # 2. Loss
    loss = loss_fn(pred, y_train)

    # 3. Grad zerostep
    optimizer.zero_grad()

    # 4. Backward
    loss.backward()

    # 5. Optimizer step
    optimizer.step()

    # print(torch.argmax(pred, dim=1))
    
    # print(y_train)

    # print(torch.argmax(pred, dim=0))
    acc = accuracy_fn(y_train, torch.argmax(pred, dim=1))
    train_loss += loss
    train_acc += acc

  train_loss /= len(dataloader)
  train_acc /= len(dataloader)

  end_time = perf_counter()

  return train_loss, train_acc, 1000 * (end_time - start_time)


# test
def test_loop(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module, accuracy_fn, 
              device: torch.device = device):
  
  
  test_loss, test_acc = 0, 0
  start_time = perf_counter()

  model.eval()
  with torch.inference_mode():
    for x_test, y_test in dataloader:

      if device == 'cuda':
        x_test, y_test = x_test.to(device), y_test.to(device)

      # 1. Forward
      test_pred = model(x_test)
      
      # 2. Loss and accuray
      test_loss += loss_fn(test_pred, y_test)
      test_acc += accuracy_fn(y_test, torch.argmax(test_pred, dim=1))

    test_loss /= len(dataloader)
    test_acc /= len(dataloader)

  end_time = perf_counter()

  return test_loss, test_acc, 1000 * (end_time - start_time)

"""## Function for Plot Loss and Accuray """

def plot_graph(train_losses, test_losses, train_accs, test_accs):
  plt.figure(figsize = (20, 8))
  plt.subplot(1, 2, 1)
  plt.plot(range(len(train_losses)), train_losses, label = "Train Loss")
  plt.plot(range(len(test_losses)), test_losses, label = "Test Loss")
  plt.legend()
  plt.xlabel("Epoches")
  plt.ylabel("Loss")
  # plt.show()

  plt.subplot(1, 2, 2)
  plt.plot(range(len(train_accs)), train_accs, label = "Train Accuracy")
  plt.plot(range(len(test_accs)), test_accs, label = "Test Accuracy")
  plt.legend()
  plt.xlabel("Epoches")
  plt.ylabel("Accuracy")
  plt.show()

"""## Loss and Accuracy"""

loss_fn = nn.CrossEntropyLoss()

accuracy_fn = MulticlassAccuracy(num_classes = len(class_names)).to(device)

"""## Save Model"""

def save_model(MODEL_NAME, model: torch.nn.Module):

  MODEL_PATH = Path("drive/MyDrive/Course/Sem2/DLOps/")
  MODEL_PATH.mkdir(parents = True, exist_ok = True)

  # MODEL_NAME = "ad_model.pth"
  MODEL_PATH_SAVE = MODEL_PATH / MODEL_NAME

  print(f"model saved at: {MODEL_PATH_SAVE}")
  torch.save(obj = model.state_dict(), f = MODEL_PATH_SAVE)

"""# Build Model
  ### ResNet-34
  ### DenseNet-121
  ### EfficientNet-B0
  ### ConvNeXt-T

## ResNet-34
"""

resnet34 = models.resnet34(pretrained = True, progress = True)
feature_number = resnet34.fc.in_features
resnet34.fc = nn.Linear(feature_number, len(class_names)).to(device)

resnet34

"""### Train resnet-34 for some epoches"""

# init. epochs
epoches = 11

resnet34 = models.resnet34(pretrained = True, progress = True).to(device)
feature_number = resnet34.fc.in_features
resnet34.fc = nn.Linear(feature_number, len(class_names)).to(device)


optimizer = torch.optim.Adam(params = resnet34.parameters(), lr = 1e-3)
resnet34_train_loss, resnet34_test_loss = [], []
resnet34_train_accs, resnet34_test_accs = [], []
resnet34_exe_train_time, resnet34_exe_test_time = [], []

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):

  train_loss, train_acc, ex_train_time = train_loop(model = resnet34, dataloader = train_dataloader,
                                    loss_fn = loss_fn, optimizer = optimizer,
                                    accuracy_fn = accuracy_fn, device = device)
  
  test_loss, test_acc, ex_test_time = test_loop(model = resnet34, dataloader = test_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  
  resnet34_train_loss.append(train_loss.item())
  resnet34_test_loss.append(test_loss.item())
  resnet34_train_accs.append(train_acc.item())
  resnet34_test_accs.append(test_acc.item())
  resnet34_exe_train_time.append(ex_train_time)
  resnet34_exe_test_time.append(ex_test_time)

  print(f"Epoch: {epoch+1}  Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Train Accuray: {train_acc:.4f} | Test Accuracy: {test_acc:.4f}")

plot_graph(resnet34_train_loss, resnet34_test_loss, resnet34_train_accs, resnet34_test_accs)

save_model('cifair100_resnet34.pth', resnet34)
print("Model saved")

# del variables
gc.collect()
torch.cuda.empty_cache()

"""### Load model"""

resnet34 = models.resnet34(pretrained = True, progress = True).to(device)
feature_number = resnet34.fc.in_features
resnet34.fc = nn.Linear(feature_number, len(class_names)).to(device)

# path
MODEL_PATH = Path("drive/MyDrive/Course/Sem2/DLOps/")

MODEL_NAME = "cifair100_resnet34.pth"
MODEL_PATH_SAVE = MODEL_PATH / MODEL_NAME

if torch.cuda.is_available() == False:
  resnet34.load_state_dict(torch.load(f = MODEL_PATH_SAVE, map_location=torch.device('cpu')))
else:
  resnet34.load_state_dict(torch.load(f = MODEL_PATH_SAVE))

# model_18.state_dict()

resnet34 = resnet34.to(device)
optimizer = torch.optim.Adam(params = resnet34.parameters(), lr = 1e-3)

"""### TorchScript"""

# torchscript resnet34
scripted_resnet34 = torch.jit.script(resnet34)

scripted_resnet34_test_loss, scripted_resnet34_test_accs = [], []
scripted_resnet34_exe_test_time = []

epoches = 11

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):
  
  test_loss, test_acc, ex_test_time = test_loop(model = scripted_resnet34, dataloader = test_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  

  scripted_resnet34_test_loss.append(test_loss.item())
  scripted_resnet34_test_accs.append(test_acc.item())
  scripted_resnet34_exe_test_time.append(ex_test_time)

"""### Frozen Script"""

# frozen script
frozen_resnet34 = torch.jit.optimize_for_inference(scripted_resnet34)

frozen_resnet34_test_loss, frozen_resnet34_test_accs = [], []
frozen_resnet34_exe_test_time = []

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):
  
  test_loss, test_acc, ex_test_time = test_loop(model = scripted_resnet34, dataloader = test_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  
  frozen_resnet34_test_loss.append(test_loss.item())
  frozen_resnet34_test_accs.append(test_acc.item())
  frozen_resnet34_exe_test_time.append(ex_test_time)

"""### Time comparision"""

resnet34_exe_test_time = np.array(resnet34_exe_test_time)

scripted_resnet34_exe_test_time = np.array(scripted_resnet34_exe_test_time)

frozen_resnet34_exe_test_time = np.array(frozen_resnet34_exe_test_time)

print(f'Torch ResNet34: {np.mean(resnet34_exe_test_time)}')
print(f'TorchScript ResNet34: {np.mean(scripted_resnet34_exe_test_time)}')
print(f'FrozenScript ResNet34: {np.mean(frozen_resnet34_exe_test_time)}')

# del variables
gc.collect()
torch.cuda.empty_cache()

"""### Tensorboard"""

with torch.profiler.profile(
        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),
        on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs/resnet34'),
        record_shapes=True,
        profile_memory=True,
        with_stack=True
) as prof:
    for step, batch_data in enumerate(train_dataloader):
        if step >= (1 + 1 + 3) * 2:
            break
        train(batch_data, resnet34, loss_fn, optimizer)
        prof.step()

# Commented out IPython magic to ensure Python compatibility.
try:
#   %load_ext tensorboard
except:
  print("reload tensor board")
else:
#   %reload_ext tensorboard
#%tensorboard --logdir log
# %tensorboard --logdir ./logs

# del variables
gc.collect()
torch.cuda.empty_cache()

"""### ONNX on resnet34"""

resnet34.eval()
test_data = []
for x, y in test_dataset:
    # x = x.to(device)
      test_data.append(x.tolist())
    # break

# data = tuple(test_data)
data = np.array(test_data)
tensor_data = torch.FloatTensor(data).to(device)
tensor_data = tensor_data[:1000]

input_names = [ "actual_input" ]
output_names = [ "output" ]

torch.onnx.export(resnet34,
                 tensor_data,
                 "resnet34.onnx",
                 verbose=False,
                 input_names=input_names,
                 output_names=output_names,
                 export_params=True,
                 )

import onnx

onnx_model = onnx.load("resnet34.onnx")
onnx.checker.check_model(onnx_model)

# import onnxruntime

# ort_session = onnxruntime.InferenceSession("resnet34.onnx")

# def to_numpy(tensor):
#     return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

# # compute ONNX Runtime output prediction
# ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}
# ort_outs = ort_session.run(None, ort_inputs)

# # compare ONNX Runtime and PyTorch results
# torch_out = resnet34(x) #torch.randn(1, 3, 224, 224)
# np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)



ort_session = onnxruntime.InferenceSession("resnet34.onnx")

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

# compute ONNX Runtime output prediction
ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(tensor_data)}
ort_outs = ort_session.run(None, ort_inputs)

# compare ONNX Runtime and PyTorch results
torch_out = resnet34(tensor_data) #torch.randn(1, 3, 224, 224)
onnx_resnet34 = np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)

"""### ONNX quantized"""

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!python -m onnxoptimizer resnet34.onnx resnet34_opt.onnx

!pip install -q onnxoptimizer
def quantize_onnx_model(onnx_model_path, quantized_model_path):
    onnx_opt_model = onnx.load(onnx_model_path)
    quantize_dynamic(onnx_model_path,
                     quantized_model_path,
                     weight_type=QuantType.QUInt8) #QInt8

    print(f"quantized model saved to:{quantized_model_path}")

quantize_onnx_model('resnet34_opt.onnx', 'resnet34_opt_quant.onnx')


print('Original model size (MB):', os.path.getsize("drive/MyDrive/Course/Sem2/DLOps/cifair100_resnet34.pth")/(1024*1024))
print('ONNX full precision model size (MB):', os.path.getsize("resnet34_opt.onnx")/(1024*1024))
print('ONNX quantized model size (MB):', os.path.getsize("resnet34_opt_quant.onnx")/(1024*1024))

import time

dummy_input = tensor_data

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

def time_ort_model_evaluation(model_path):
    sess_options = onnxruntime.SessionOptions()
    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
    session = onnxruntime.InferenceSession(model_path, sess_options)

    time_per_inference = []
    for _ in range(10):
        # dummy_input = torch.randn(1, 3, 224, 224)
        # compute ONNX Runtime output prediction
        ort_inputs = {session.get_inputs()[0].name: to_numpy(dummy_input)}
        start = perf_counter()
        session.run(None, ort_inputs)
        time_per_inference.append((1000 * (perf_counter() - start)))

    return np.mean(time_per_inference)

print('Average runtime of ONNX Model in GPU: ' + str(time_ort_model_evaluation('resnet34.onnx')))
print('Average runtime of ONNX Quantized Model in GPU: ' + str(time_ort_model_evaluation('resnet34_opt_quant.onnx')))

# del variables
gc.collect()
torch.cuda.empty_cache()

"""## DenseNet-121"""

densenet121 = models.densenet121(weights = 'IMAGENET1K_V1', progress = True).to(device)
densenet121.classifier = nn.Linear(1024, len(class_names)).to(device)

densenet121

"""### Train densenet-121 for some epoches"""

# init. epochs
epoches = 11

densenet121 = models.densenet121(weights = 'IMAGENET1K_V1', progress = True).to(device)
densenet121 = models.densenet121(weights = 'IMAGENET1K_V1', progress = True).to(device)
densenet121.classifier = nn.Linear(1024, len(class_names)).to(device)


optimizer = torch.optim.Adam(params = densenet121.parameters(), lr = 1e-3)
densenet121_train_loss, densenet121_test_loss = [], []
densenet121_train_accs, densenet121_test_accs = [], []
densenet121_exe_train_time, densenet121_exe_test_time = [], []

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):

  train_loss, train_acc, ex_train_time = train_loop(model = densenet121, dataloader = train_dataloader,
                                    loss_fn = loss_fn, optimizer = optimizer,
                                    accuracy_fn = accuracy_fn, device = device)
  
  test_loss, test_acc, ex_test_time = test_loop(model = densenet121, dataloader = test_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  
  densenet121_train_loss.append(train_loss.item())
  densenet121_test_loss.append(test_loss.item())
  densenet121_train_accs.append(train_acc.item())
  densenet121_test_accs.append(test_acc.item())
  densenet121_exe_train_time.append(ex_train_time)
  densenet121_exe_test_time.append(ex_test_time)


  print(f"Epoch: {epoch+1}  Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Train Accuray: {train_acc:.4f} | Test Accuracy: {test_acc:.4f}")

plot_graph(densenet121_train_loss, densenet121_test_loss, densenet121_train_accs, densenet121_test_accs)

save_model('cifair100_densenet121.pth', densenet121)
print("Model saved")

# del variables
gc.collect()
torch.cuda.empty_cache()

"""### Load Model"""

densenet121 = models.densenet121(weights = 'IMAGENET1K_V1', progress = True).to(device)
densenet121.classifier = nn.Linear(1024, len(class_names)).to(device)

# path
MODEL_PATH = Path("drive/MyDrive/Course/Sem2/DLOps/")

MODEL_NAME = "cifair100_densenet121.pth"
MODEL_PATH_SAVE = MODEL_PATH / MODEL_NAME

if torch.cuda.is_available() == False:
  densenet121.load_state_dict(torch.load(f = MODEL_PATH_SAVE, map_location=torch.device('cpu')))
else:
  densenet121.load_state_dict(torch.load(f = MODEL_PATH_SAVE))

# model_18.state_dict()

densenet121 = densenet121.to(device)
optimizer = torch.optim.Adam(params = densenet121.parameters(), lr = 1e-3)

"""### TorchScript"""

# torchscript resnet34
scripted_densenet121 = torch.jit.script(densenet121)

scripted_densenet121_test_loss, scripted_densenet121_test_accs = [], []
scripted_densenet121_exe_test_time = []

epoches = 11

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):
  
  test_loss, test_acc, ex_test_time = test_loop(model = scripted_densenet121, dataloader = test_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  

  scripted_densenet121_test_loss.append(test_loss.item())
  scripted_densenet121_test_accs.append(test_acc.item())
  scripted_densenet121_exe_test_time.append(ex_test_time)

"""### Frozen Script"""

# frozen script
frozen_densenet121 = torch.jit.optimize_for_inference(scripted_densenet121)

frozen_densenet121_test_loss, frozen_densenet121_test_accs = [], []
frozen_densenet121_exe_test_time = []

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):
  
  test_loss, test_acc, ex_test_time = test_loop(model = frozen_densenet121, dataloader = test_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  
  frozen_densenet121_test_loss.append(test_loss.item())
  frozen_densenet121_test_accs.append(test_acc.item())
  frozen_densenet121_exe_test_time.append(ex_test_time)

"""### Time comparision"""

densenet121_exe_test_time = np.array(densenet121_exe_test_time)

scripted_densenet121_exe_test_time = np.array(scripted_densenet121_exe_test_time)

frozen_densenet121_exe_test_time = np.array(frozen_densenet121_exe_test_time)

print(f'Torch DenseNet121: {np.mean(densenet121_exe_test_time)}')
print(f'TorchScript DenseNet121: {np.mean(scripted_densenet121_exe_test_time)}')
print(f'FrozenScript DenseNet121: {np.mean(frozen_densenet121_exe_test_time)}')

# del variables
gc.collect()
torch.cuda.empty_cache()

"""### Tensorboard"""

with torch.profiler.profile(
        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),
        on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs/densenet121'),
        record_shapes=True,
        profile_memory=True,
        with_stack=True
) as prof:
    for step, batch_data in enumerate(train_dataloader):
        if step >= (1 + 1 + 3) * 2:
            break
        train(batch_data, densenet121, loss_fn, optimizer)
        prof.step()

# Commented out IPython magic to ensure Python compatibility.
try:
#   %load_ext tensorboard
except:
  print("reload tensor board")
else:
#   %reload_ext tensorboard
#%tensorboard --logdir log
# %tensorboard --logdir ./logs

# del variables
gc.collect()
torch.cuda.empty_cache()

"""### ONNX on DenseNet-121"""

# tensor_data = tensor_data[:1000]

densenet121.eval()
input_names = [ "actual_input" ]
output_names = [ "output" ]

torch.onnx.export(densenet121,
                 tensor_data,
                 "densenet121.onnx",
                 verbose=False,
                 input_names=input_names,
                 output_names=output_names,
                 export_params=True,
                 )

onnx_model = onnx.load("densenet121.onnx")
onnx.checker.check_model(onnx_model)

ort_session = onnxruntime.InferenceSession("densenet121.onnx")

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

# compute ONNX Runtime output prediction
ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(tensor_data)}
ort_outs = ort_session.run(None, ort_inputs)

# compare ONNX Runtime and PyTorch results
torch_out = densenet121(tensor_data) #torch.randn(1, 3, 224, 224)
onnx_densenet121 = np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)

"""### ONNX quantized"""

!python -m onnxoptimizer densenet121.onnx densenet121_opt.onnx

quantize_onnx_model('densenet121_opt.onnx', 'densenet121_opt_quant.onnx')

print('Original model size (MB):', os.path.getsize("drive/MyDrive/Course/Sem2/DLOps/cifair100_densenet121.pth")/(1024*1024))
print('ONNX full precision model size (MB):', os.path.getsize("densenet121_opt.onnx")/(1024*1024))
print('ONNX quantized model size (MB):', os.path.getsize("densenet121_opt_quant.onnx")/(1024*1024))

print('Average runtime of ONNX Model in GPU: ' + str(time_ort_model_evaluation('densenet121.onnx')))
print('Average runtime of ONNX Quantized Model in GPU: ' + str(time_ort_model_evaluation('densenet121_opt_quant.onnx')))

# del variables
gc.collect()
torch.cuda.empty_cache()

"""## EfficientNet-B0"""

efficientnetB0 = models.efficientnet_b0(weights = 'IMAGENET1K_V1', progress = True).to(device)

efficientnetB0.classifier = nn.Linear(1280, len(class_names)).to(device)

efficientnetB0

"""### Train efficientnet-b0 for some epoches

"""

# init. epochs
epoches = 11

efficientnetB0 = models.efficientnet_b0(weights = 'IMAGENET1K_V1', progress = True).to(device)
efficientnetB0.classifier = nn.Linear(1280, len(class_names)).to(device)


optimizer = torch.optim.Adam(params = efficientnetB0.parameters(), lr = 1e-3)
efficientnetB0_train_loss, efficientnetB0_test_loss = [], []
efficientnetB0_train_accs, efficientnetB0_test_accs = [], []
efficientnetB0_exe_train_time, efficientnetB0_exe_test_time = [], []

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):

  train_loss, train_acc, ex_train_time = train_loop(model = efficientnetB0, dataloader = train_dataloader,
                                    loss_fn = loss_fn, optimizer = optimizer,
                                    accuracy_fn = accuracy_fn, device = device)
  
  test_loss, test_acc, ex_test_time = test_loop(model = efficientnetB0, dataloader = test_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  
  efficientnetB0_train_loss.append(train_loss.item())
  efficientnetB0_test_loss.append(test_loss.item())
  efficientnetB0_train_accs.append(train_acc.item())
  efficientnetB0_test_accs.append(test_acc.item())
  efficientnetB0_exe_train_time.append(ex_train_time)
  efficientnetB0_exe_test_time.append(ex_test_time)

  print(f"Epoch: {epoch+1}  Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Train Accuray: {train_acc:.4f} | Test Accuracy: {test_acc:.4f}")

plot_graph(efficientnetB0_train_loss, efficientnetB0_test_loss, efficientnetB0_train_accs, efficientnetB0_test_accs)

save_model('cifair100_efficientnetB0.pth', efficientnetB0)
print("Model saved")

# del variables
gc.collect()
torch.cuda.empty_cache()

"""### Load Model"""

efficientnetB0 = models.efficientnet_b0(weights = 'IMAGENET1K_V1', progress = True).to(device)

efficientnetB0.classifier = nn.Linear(1280, len(class_names)).to(device)

# path
MODEL_PATH = Path("drive/MyDrive/Course/Sem2/DLOps/")

MODEL_NAME = "cifair100_efficientnetB0.pth"
MODEL_PATH_SAVE = MODEL_PATH / MODEL_NAME

if torch.cuda.is_available() == False:
  efficientnetB0.load_state_dict(torch.load(f = MODEL_PATH_SAVE, map_location=torch.device('cpu')))
else:
  efficientnetB0.load_state_dict(torch.load(f = MODEL_PATH_SAVE))

# model_18.state_dict()

efficientnetB0 = efficientnetB0.to(device)
optimizer = torch.optim.Adam(params = efficientnetB0.parameters(), lr = 1e-3)

"""### TorchScript"""

# torchscript resnet34
scripted_efficientnetB0 = torch.jit.script(efficientnetB0)

scripted_efficientnetB0_test_loss, scripted_efficientnetB0_test_accs = [], []
scripted_efficientnetB0_exe_test_time = []

epoches = 11

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):
  
  test_loss, test_acc, ex_test_time = test_loop(model = scripted_efficientnetB0, dataloader = test_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  

  scripted_efficientnetB0_test_loss.append(test_loss.item())
  scripted_efficientnetB0_test_accs.append(test_acc.item())
  scripted_efficientnetB0_exe_test_time.append(ex_test_time)

"""### Frozen Script"""

# frozen script
frozen_efficientnetB0 = torch.jit.optimize_for_inference(scripted_efficientnetB0)

frozen_efficientnetB0_test_loss, frozen_efficientnetB0_test_accs = [], []
frozen_efficientnetB0_exe_test_time = []

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):
  
  test_loss, test_acc, ex_test_time = test_loop(model = frozen_efficientnetB0, dataloader = test_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  
  frozen_efficientnetB0_test_loss.append(test_loss.item())
  frozen_efficientnetB0_test_accs.append(test_acc.item())
  frozen_efficientnetB0_exe_test_time.append(ex_test_time)

"""### Time comparision"""

efficientnetB0_exe_test_time = np.array(efficientnetB0_exe_test_time)

scripted_efficientnetB0_exe_test_time = np.array(scripted_efficientnetB0_exe_test_time)

frozen_efficientnetB0_exe_test_time = np.array(frozen_efficientnetB0_exe_test_time)

print(f'Torch EfficientnetB0: {np.mean(efficientnetB0_exe_test_time)}')
print(f'TorchScript EfficientnetB0: {np.mean(scripted_efficientnetB0_exe_test_time)}')
print(f'FrozenScript EfficientnetB0: {np.mean(frozen_efficientnetB0_exe_test_time)}')

# del variables
gc.collect()
torch.cuda.empty_cache()

"""### Tensorboard"""

with torch.profiler.profile(
        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),
        on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs/efficientnetB0'),
        record_shapes=True,
        profile_memory=True,
        with_stack=True
) as prof:
    for step, batch_data in enumerate(train_dataloader):
        if step >= (1 + 1 + 3) * 2:
            break
        train(batch_data, efficientnetB0, loss_fn, optimizer)
        prof.step()

# Commented out IPython magic to ensure Python compatibility.
try:
#   %load_ext tensorboard
except:
  print("reload tensor board")
else:
#   %reload_ext tensorboard
#%tensorboard --logdir log
# %tensorboard --logdir ./logs

# del variables
gc.collect()
torch.cuda.empty_cache()

"""### ONNX on EfficientNet-b0"""

efficientnetB0.eval()
input_names = [ "actual_input" ]
output_names = [ "output" ]

torch.onnx.export(densenet121,
                 tensor_data,
                 "efficientnetB0.onnx",
                 verbose=False,
                 input_names=input_names,
                 output_names=output_names,
                 export_params=True,
                 )

onnx_model = onnx.load("efficientnetB0.onnx")
onnx.checker.check_model(onnx_model)

ort_session = onnxruntime.InferenceSession("efficientnetB0.onnx")

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

# compute ONNX Runtime output prediction
ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(tensor_data)}
ort_outs = ort_session.run(None, ort_inputs)

# compare ONNX Runtime and PyTorch results
torch_out = efficientnetB0(tensor_data) #torch.randn(1, 3, 224, 224)
onnx_efficientnetB0 = np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)

"""### ONNX quantized"""

!python -m onnxoptimizer efficientnetB0.onnx efficientnetB0_opt.onnx

quantize_onnx_model('efficientnetB0_opt.onnx', 'efficientnetB0_opt_quant.onnx')

print('Original model size (MB):', os.path.getsize("drive/MyDrive/Course/Sem2/DLOps/cifair100_efficientnetB0.pth")/(1024*1024))
print('ONNX full precision model size (MB):', os.path.getsize("efficientnetB0_opt.onnx")/(1024*1024))
print('ONNX quantized model size (MB):', os.path.getsize("efficientnetB0_opt_quant.onnx")/(1024*1024))

print('Average runtime of ONNX Model in GPU: ' + str(time_ort_model_evaluation('efficientnetB0.onnx')))
print('Average runtime of ONNX Quantized Model in GPU: ' + str(time_ort_model_evaluation('efficientnetB0_opt_quant.onnx')))

# del variables
gc.collect()
torch.cuda.empty_cache()

"""## ConvNeXt-T"""

convnext_tiny = models.convnext_tiny(weights = 'IMAGENET1K_V1', progress = True).to(device)
# convnext_tiny.classifier = nn.Linear(768, len(class_names)).to(device)
# in_features = convnext_tiny.classifier[1].in_features
# convnext_tiny.classifier[1] = nn.Linear(in_features, len(class_names)).to(device)

# # Freeze the pre-trained layers
# for param in convnext_tiny.parameters():
#     param.requires_grad = False

# # Enable gradients for the new layer
# for param in convnext_tiny.classifier[0].parameters():
#     param.requires_grad = True
# in_features = convnext_tiny.classifier[1].in_features
new_classifier = nn.Sequential(
    nn.AdaptiveAvgPool2d((1, 1)),
    nn.Flatten(),
    nn.Linear(768, len(class_names))
)

# Replace the classifier with the new one
convnext_tiny.classifier = new_classifier.to(device)

# Freeze the pre-trained layers
for param in convnext_tiny.parameters():
    param.requires_grad = False

# Enable gradients for the new layer
for param in convnext_tiny.classifier[2].parameters():
    param.requires_grad = True

convnext_tiny

"""### Train ConvNext-T for some epoches"""

# init. epochs
epoches = 5

convnext_tiny = models.convnext_tiny(weights = 'IMAGENET1K_V1', progress = True).to(device)
new_classifier = nn.Sequential(
    nn.AdaptiveAvgPool2d((1, 1)),
    nn.Flatten(),
    nn.Linear(768, len(class_names))
)

# Replace the classifier with the new one
convnext_tiny.classifier = new_classifier.to(device)

# Freeze the pre-trained layers
for param in convnext_tiny.parameters():
    param.requires_grad = False

# Enable gradients for the new layer
for param in convnext_tiny.classifier[2].parameters():
    param.requires_grad = True


optimizer = torch.optim.Adam(params = convnext_tiny.parameters(), lr = 1e-3)
convnext_tiny_train_loss, convnext_tiny_test_loss = [], []
convnext_tiny_train_accs, convnext_tiny_test_accs = [], []
convnext_tiny_exe_train_time, convnext_tiny_exe_test_time = [], []

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):

  train_loss, train_acc, ex_train_time = train_loop(model = convnext_tiny, dataloader = train_dataloader,
                                    loss_fn = loss_fn, optimizer = optimizer,
                                    accuracy_fn = accuracy_fn, device = device)
  
  test_loss, test_acc, ex_test_time = test_loop(model = convnext_tiny, dataloader = test_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  
  convnext_tiny_train_loss.append(train_loss.item())
  convnext_tiny_test_loss.append(test_loss.item())
  convnext_tiny_train_accs.append(train_acc.item())
  convnext_tiny_test_accs.append(test_acc.item())
  convnext_tiny_exe_train_time.append(ex_train_time)
  convnext_tiny_exe_test_time.append(ex_test_time)

  print(f"Epoch: {epoch+1}  Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Train Accuray: {train_acc:.4f} | Test Accuracy: {test_acc:.4f}")

plot_graph(convnext_tiny_train_loss, convnext_tiny_test_loss, convnext_tiny_train_accs, convnext_tiny_test_accs)

save_model('cifair100_convnext_tiny.pth', convnext_tiny)
print("Model saved")

# del variables
gc.collect()
torch.cuda.empty_cache()

"""### Load Model"""

convnext_tiny = models.convnext_tiny(weights = 'IMAGENET1K_V1', progress = True).to(device)
new_classifier = nn.Sequential(
    nn.AdaptiveAvgPool2d((1, 1)),
    nn.Flatten(),
    nn.Linear(768, len(class_names))
)

# path
MODEL_PATH = Path("drive/MyDrive/Course/Sem2/DLOps/")

MODEL_NAME = "cifair100_convnext_tiny.pth"
MODEL_PATH_SAVE = MODEL_PATH / MODEL_NAME

if torch.cuda.is_available() == False:
  convnext_tiny.load_state_dict(torch.load(f = MODEL_PATH_SAVE, map_location=torch.device('cpu')))
else:
  convnext_tiny.load_state_dict(torch.load(f = MODEL_PATH_SAVE))


convnext_tiny = convnext_tiny.to(device)
optimizer = torch.optim.Adam(params = convnext_tiny.parameters(), lr = 1e-3)

"""### TorchScript"""

# torchscript convnext_tiny
scripted_convnext_tiny = torch.jit.script(convnext_tiny)

scripted_convnext_tiny_test_loss, scripted_convnext_tiny_test_accs = [], []
scripted_convnext_tiny_exe_test_time = []

epoches = 5

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):
  
  test_loss, test_acc, ex_test_time = test_loop(model = scripted_convnext_tiny, dataloader = test_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  

  scripted_convnext_tiny_test_loss.append(test_loss.item())
  scripted_convnext_tiny_test_accs.append(test_acc.item())
  scripted_convnext_tiny_exe_test_time.append(ex_test_time)

"""### Frozen Script"""

# frozen script
frozen_convnext_tiny = torch.jit.optimize_for_inference(scripted_convnext_tiny)

frozen_convnext_tiny_test_loss, frozen_convnext_tiny_test_accs = [], []
frozen_convnext_tiny_exe_test_time = []

torch.manual_seed(64)
torch.cuda.manual_seed(64)
for epoch in tqdm(range(epoches)):
  
  test_loss, test_acc, ex_test_time = test_loop(model = frozen_convnext_tiny, dataloader = test_dataloader,
                                  loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                  device = device)
  
  frozen_convnext_tiny_test_loss.append(test_loss.item())
  frozen_convnext_tiny_test_accs.append(test_acc.item())
  frozen_convnext_tiny_exe_test_time.append(ex_test_time)

"""### Time comparision"""

convnext_tiny_exe_test_time = np.array(convnext_tiny_exe_test_time)

scripted_convnext_tiny_exe_test_time = np.array(scripted_convnext_tiny_exe_test_time)

frozen_convnext_tiny_exe_test_time = np.array(frozen_convnext_tiny_exe_test_time)

print(f'Torch Convnext_tiny: {np.mean(convnext_tiny_exe_test_time)}')
print(f'TorchScript Convnext_tiny: {np.mean(scripted_convnext_tiny_exe_test_time)}')
print(f'FrozenScript Convnext_tiny: {np.mean(frozen_convnext_tiny_exe_test_time)}')

# del variables
gc.collect()
torch.cuda.empty_cache()

"""### Tensorboard"""

with torch.profiler.profile(
        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),
        on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs/convnext_tiny'),
        record_shapes=True,
        profile_memory=True,
        with_stack=True
) as prof:
    for step, batch_data in enumerate(train_dataloader):
        if step >= (1 + 1 + 3) * 2:
            break
        train(batch_data, convnext_tiny, loss_fn, optimizer)
        prof.step()

# Commented out IPython magic to ensure Python compatibility.
try:
#   %load_ext tensorboard
except:
  print("reload tensor board")
else:
#   %reload_ext tensorboard
#%tensorboard --logdir log
# %tensorboard --logdir ./logs

# del variables
gc.collect()
torch.cuda.empty_cache()

"""### ONNX on ConvNeXt-T"""

convnext_tiny.eval()
input_names = [ "actual_input" ]
output_names = [ "output" ]

torch.onnx.export(convnext_tiny,
                 tensor_data,
                 "convnext_tiny.onnx",
                 verbose=False,
                 input_names=input_names,
                 output_names=output_names,
                 export_params=True,
                 )

onnx_model = onnx.load("convnext_tiny.onnx")
onnx.checker.check_model(onnx_model)

ort_session = onnxruntime.InferenceSession("convnext_tiny.onnx")

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

# compute ONNX Runtime output prediction
ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(tensor_data)}
ort_outs = ort_session.run(None, ort_inputs)

# compare ONNX Runtime and PyTorch results
torch_out = convnext_tiny(tensor_data) #torch.randn(1, 3, 224, 224)
onnx_convnext_tiny = np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)

"""### ONNX quantized"""

!python -m onnxoptimizer convnext_tiny.onnx convnext_tiny_opt.onnx

quantize_onnx_model('convnext_tiny_opt.onnx', 'convnext_tiny_opt_quant.onnx')

print('Original model size (MB):', os.path.getsize("drive/MyDrive/Course/Sem2/DLOps/cifair100_convnext_tiny.pth")/(1024*1024))
print('ONNX full precision model size (MB):', os.path.getsize("convnext_tiny_opt.onnx")/(1024*1024))
print('ONNX quantized model size (MB):', os.path.getsize("convnext_tiny_opt_quant.onnx")/(1024*1024))

convnext_tiny
print('Average runtime of ONNX Model in GPU: ' + str(time_ort_model_evaluation('convnext_tiny.onnx')))
print('Average runtime of ONNX Quantized Model in GPU: ' + str(time_ort_model_evaluation('convnext_tiny_opt_quant.onnx')))

# del variables
gc.collect()
torch.cuda.empty_cache()