# -*- coding: utf-8 -*-
"""M22CS061_Lab_Assignment_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ylygD_NXftivtHdZCSL4hWDq9XZk4huD
"""

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
import unicodedata
import string
from __future__ import unicode_literals, print_function, division
from io import open
import glob
import os
from google.colab import drive
drive.mount('/content/drive/')

import unicodedata
import string
from tqdm.auto import tqdm

!pip install -q torchmetrics
from torchmetrics.classification import MulticlassAccuracy

import matplotlib.pyplot as plt
import gc

"""## device agnostic code"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

"""## Upload data set from drive"""

def findFiles(path): return glob.glob(path)

print(findFiles('drive/MyDrive/Course/Sem2/DLOps/data/names/*.txt'))

all_letters = string.ascii_letters + " .,;'"
n_letters = len(all_letters)

# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427
# pratik sir code
def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn'
        and c in all_letters
    )

# print(unicodeToAscii('Ślusàrski'))
print(unicodeToAscii('Koury'))

# Build the category_lines dictionary, a list of names per language
category_lines = {}
all_categories = []

# Read a file and split into lines
def readLines(filename):
    lines = open(filename, encoding='utf-8').read().strip().split('\n')
    return [unicodeToAscii(line) for line in lines]

for filename in findFiles('drive/MyDrive/Course/Sem2/DLOps/data/names/*.txt'):
    category = os.path.splitext(os.path.basename(filename))[0]
    all_categories.append(category)
    lines = readLines(filename)
    category_lines[category] = lines

n_categories = len(all_categories)

all_letters, all_categories

"""## One hot encoding"""

# # Find letter index from all_letters, e.g. "a" = 0
# def letterToIndex(letter):
#     return 

# Just for demonstration, turn a letter into a <1 x n_letters> Tensor
def letterToTensor(letter):
    tensor = torch.zeros(1, n_letters)
    tensor[0][all_letters.find(letter)] = 1
    return tensor

# Turn a line into a <line_length x 1 x n_letters>,
# or an array of one-hot letter vectors
def lineToTensor(line):
    tensor = torch.zeros(len(line), 1, n_letters)
    for li, letter in enumerate(line):
        tensor[li][0][all_letters.find(letter)] = 1
    return tensor

print(letterToTensor('J'))

print(lineToTensor('Josh').size())

# all_cat

import random
def randomChoice(l):
    return l[random.randint(0, len(l) - 1)]

def randomTrainingExample():
    category = randomChoice(all_categories)
    # print(f'cat: {category}')
    line = randomChoice(category_lines[category])
    # print(f'line: {line}')
    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)
    # print(f'category_tensor: {category_tensor}')
    line_tensor = lineToTensor(line)
    # print(f'line_tensor: {line_tensor}')
    return category, line, category_tensor, line_tensor

for i in range(10):
    category, line, category_tensor, line_tensor = randomTrainingExample()
    print('category =', category, '/ line =', line)

len(category_lines)

"""## get dataset ready for dataloader"""

n_iters = 100000


data_list = []
for iter in range(1, n_iters + 1):
  category, line, category_tensor, line_tensor = randomTrainingExample()

  data = (line_tensor.to(device), category_tensor.to(device))
  data_list.append(data)

len(data_list)

"""## make dataset of data list"""

datasets = torch.utils.data.ConcatDataset([data_list])
len(datasets)

datasets

"""## Task 1

## Split data in train, test and val
"""

# split datasets in train, test, val
train_set, test_val_set = torch.utils.data.random_split(datasets, [80000, 20000])
test_set, val_set = torch.utils.data.random_split(test_val_set, [10000, 10000])

len(train_set), len(val_set), len(test_set)

"""## Make data loader"""

BATCH_SIZE = 32
train_dataloader = DataLoader(dataset = train_set, batch_size = BATCH_SIZE,
                              shuffle = True)
val_dataloader = DataLoader(dataset = val_set, batch_size = BATCH_SIZE,
                              shuffle = True)
test_dataloader = DataLoader(dataset = test_set, batch_size = BATCH_SIZE,
                              shuffle = False)

len(train_dataloader), len(test_dataloader), len(val_dataloader)

"""## Build model"""

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()

        self.hidden_size = hidden_size

        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        self.main_layer = nn.Sequential(
                                  nn.Linear(input_size + hidden_size, output_size),
                                  nn.LogSoftmax(dim=1)
                                )

    def forward(self, input, hidden):
        combined = torch.cat((input, hidden), 1)
        hidden = self.i2h(combined)
        output = self.main_layer(combined)
        # output = self.softmax(output)
        return output, hidden

    def init_Hidden(self):
        return torch.zeros(1, self.hidden_size)

n_hidden = 128
model_rnn = RNN(n_letters, n_hidden, n_categories).to(device)
model_rnn

input = letterToTensor('A')
hidden = torch.zeros(1, n_hidden)

output, next_hidden = model_rnn(input, hidden)

input = lineToTensor('Albert')
hidden = torch.zeros(1, n_hidden)

output, next_hidden = model_rnn(input[0], hidden)
print(output)

"""## get max probability output"""

# get data value from softmax max value
def categoryFromOutput(output):
    top_n, top_i = output.topk(1)
    category_i = top_i[0].item()
    return all_categories[category_i], category_i

print(categoryFromOutput(output))

# for x_train, y_train in enumerate(train_set):
for x_train, y_train in train_set:
  print(x_train, y_train)
  break

"""## train and test function(loop)"""

# train
def train_loop(model: torch.nn.Module, dataset: torch.utils.data.Dataset,
               loss_fn : torch.nn.Module, optimizer: torch.optim.Optimizer,
               lr: float, accuracy_fn, device: torch.device = device):
  
  train_loss, train_acc = 0, 0 
  hidden = model.init_Hidden()


  model.zero_grad()
  x_train, y_train = dataset

    
  if device == 'cuda':
    x_train, y_train = x_train.to(device), y_train.to(device)

  
  model.train()

  # 1. Forward
  for i in range(x_train.size()[0]):
      pred, hidden = model(x_train[i], hidden)

  # 2. Loss
  loss = loss_fn(pred, y_train)


  # Add parameters' gradients to their values, multiplied by learning rate
  # for para in model.parameters():
  #     para.data.add_(para.grad.data, alpha=-lr)

  # 3. Grad zerostep
  optimizer.zero_grad()

  # 4. Backward
  loss.backward()

  # 5. Optimizer step
  optimizer.step()

  # print(torch.argmax(pred, dim=1))
  
  # print(y_train)

  # print(torch.argmax(pred, dim=0))
  acc = (y_train == torch.argmax(pred, dim=1)).to(device)
  train_loss += loss
  train_acc += acc

  # print(train_loss, train_acc)

  return train_loss.item(), train_acc.item()


# validation_set
def validation_loop(model: torch.nn.Module, dataset: torch.utils.data.Dataset,
                    loss_fn : torch.nn.Module, optimizer: torch.optim.Optimizer,
                    lr: float, accuracy_fn, device: torch.device = device):
  
  val_loss, val_acc = 0, 0 

  model.zero_grad()

  hidden = model.init_Hidden()

  x_val, y_val = dataset

    
  if device == 'cuda':
    x_val, y_val = x_val.to(device), y_val.to(device)

  
  model.train()

  # 1. Forward
  for i in range(x_val.size()[0]):
      pred, hidden = model(x_val[i], hidden)

  # 2. Loss
  loss = loss_fn(pred, y_val)

  
  # # 3. Grad zerostep
  # optimizer.zero_grad()

  # 4. Backward
  loss.backward()

  # Add parameters' gradients to their values, multiplied by learning rate
  for para in model.parameters():
      para.data.add_(para.grad.data, alpha=-lr)

  # # 5. Optimizer step
  # optimizer.step()

  # print(torch.argmax(pred, dim=1))
  
  # print(y_train)

  # print(torch.argmax(pred, dim=0))
  acc = (y_val == torch.argmax(pred, dim=1)).to(device)
  val_loss += loss
  val_acc += acc

  return val_loss.item(), val_acc.item(), pred


# test
def test_loop(model: torch.nn.Module, dataset: torch.utils.data.Dataset,
              loss_fn: torch.nn.Module, accuracy_fn, 
              lr: float, device: torch.device = device):
  
  test_loss, test_acc = 0, 0
  hidden = model.init_Hidden()
  model.eval()
  with torch.inference_mode():
    x_test, y_test = dataset

    if device == 'cuda':
      x_test, y_test = x_test.to(device), y_test.to(device)

    # 1. Forward
    for i in range(x_test.size()[0]):
      test_pred, hidden = model(x_test[i], hidden)
    
    # 2. Loss and accuray
    test_loss += loss_fn(test_pred, y_test)
    test_acc += (y_test == torch.argmax(test_pred, dim=1)).to(device)

  return test_loss.item(), test_acc.item()

"""## Plot graph of Loss and Accuray"""

def plot_graph(train_losses, train_accs):
  plt.figure(figsize = (20, 8))
  plt.subplot(1, 2, 1)
  plt.plot(range(len(train_losses)), train_losses, label = "Loss")
  # plt.plot(range(len(test_losses)), test_losses, label = "Test Loss")
  plt.legend()
  plt.xlabel("Epoches")
  plt.ylabel("Loss")
  # plt.show()

  plt.subplot(1, 2, 2)
  plt.plot(range(len(train_accs)), train_accs, label = "Accuracy", color = 'orange')
  # plt.plot(range(len(test_accs)), test_accs, label = "Test Accuracy")
  plt.legend()
  plt.xlabel("Epoches")
  plt.ylabel("Accuracy")
  plt.show()

"""## Loss and accuracy function"""

loss_fn = nn.NLLLoss()

accuracy_fn = MulticlassAccuracy(num_classes = 18).to(device)

"""## Task 2

## train model
"""



gc.collect()
torch.cuda.empty_cache()
# epoches = 15
lr = 0.001

rnn_train_loss = []
rnn_train_acc = []


torch.manual_seed(64)
torch.cuda.manual_seed(64)
n_hidden = 128
model_rnn = RNN(n_letters, n_hidden, n_categories).to(device)

# SGD with momentum
# optimizer = torch.optim.SGD(params = resnet18.parameters(), lr = 0.1, momentum = 0.3)

# Adam as optimizer
optimizer = torch.optim.Adam(params = model_rnn.parameters(), lr = lr, 
                             betas = (0.9, 0.99))

current_loss = 0
current_acc = 0
print_every = 5000
torch.manual_seed(64)
torch.cuda.manual_seed(64)
for iter in tqdm(range(1, len(train_set))):



    train_loss, train_acc = train_loop(model = model_rnn, dataset = train_set[iter],
                                       loss_fn = loss_fn, optimizer = optimizer,
                                       lr = lr, accuracy_fn = accuracy_fn, device = device)
    
    # print()
    current_loss += train_loss
    current_acc += train_acc


    # Add current loss avg to list of losses
    if iter % print_every == 0:
        loss = current_loss / print_every
        acc = current_acc / print_every
        rnn_train_loss.append(loss)
        rnn_train_acc.append(acc)
        # print(train_loss)
        # print(train_acc)
        print(f"Epoch: {iter}  Train Loss: {train_loss:.4f} | Train Accuray: {acc:.4f}")
        # print(current_loss, current_acc)
        current_loss = 0
        current_acc = 0
        # break
# for epoch in tqdm(range(epoches)):

#   train_loss, train_acc = train_loop(model = model_rnn, dataset = train_set,
#                                     loss_fn = loss_fn, optimizer = optimizer,
#                                     lr = lr, accuracy_fn = accuracy_fn, device = device)
  
#   # test_loss, test_acc = test_loop(model = model_rnn, dataloader = test_dataloader,
#   #                                 loss_fn = loss_fn, accuracy_fn = accuracy_fn,
#   #                                 device = device)
  
#   rnn_train_loss.append(train_loss.item())
#   # resnet18_test_loss.append(test_loss.item())
#   rnn_train_accs.append(train_acc.item())
#   # resnet18_test_accs.append(test_acc.item())

#   # print(f"Epoch: {epoch+1}  Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Train Accuray: {train_acc:.4f} | Test Accuracy: {test_acc:.4f}")
#   print(f"Epoch: {epoch+1}  Train Loss: {train_loss:.4f} | Train Accuray: {train_acc:.4f}")

plot_graph(rnn_train_loss, rnn_train_acc)

"""## Hyper parameter tuning"""

print_every = 1000


# Hyperparameter
lr_rate = [0.001, 0.003, 0.005, 0.007, 0.0001] # learning rate
betas=[(0.8, 0.88),(0.9, 0.99)] # coefficients used for computing running averages of gradient and its square

best_model = RNN(n_letters, n_hidden, n_categories).to(device)
best_model_losses, best_model_acces = [], []
best_acc = 0
best_lr = None
best_betas = None

hyper_params = [(lr,b) for lr in lr_rate for b in betas]
# print(parms_combs)

cur_iter,total_iter = 1, len(lr_rate)*len(betas)

for h_p in hyper_params:
  rnn_val_loss = []
  rnn_val_acc = []
  acc = 0
  print()
  print(f"current exp | total: {cur_iter} | {total_iter}")
  print(f"Training with --- lr rate: {h_p[0]}, betas: {h_p[1]}")
  model_rnn = RNN(n_letters, n_hidden, n_categories).to(device)

  optimizer = torch.optim.Adam(params=model_rnn.parameters(), lr=h_p[0], betas=h_p[1])

  torch.manual_seed(64)
  torch.cuda.manual_seed(64)
  for iter in tqdm(range(1, len(val_set))):

    val_loss, val_acc, output = validation_loop(model = model_rnn, dataset = val_set[iter],
                                         loss_fn = loss_fn, optimizer = optimizer,
                                         lr = lr, accuracy_fn = accuracy_fn, device = device)
    
    # print()
    current_loss += val_loss
    current_acc += val_acc


    # Add current loss avg to list of losses
    if iter % (print_every-1) == 9:
        loss = current_loss / print_every
        acc = current_acc / print_every
        rnn_val_loss.append(loss)
        rnn_val_acc.append(acc)
        # print(train_loss)
        # print(train_acc)
        print(f"Epoch: {iter+1}  Train Loss: {train_loss:.4f} | Train Accuray: {acc:.4f}")
        # print(current_loss, current_acc)
        current_loss = 0
        current_acc = 0
    
  if best_acc < acc:
    best_acc = acc
    best_lr = h_p[0]
    best_betas = h_p[1]
    best_model_losses = rnn_val_loss.copy()
    best_model_acces = rnn_val_acc.copy()
    best_model.load_state_dict(model_rnn.state_dict())

  cur_iter += 1

"""## Best model Loss and Accuray plot"""

plot_graph(best_model_losses, best_model_acces)

print(f"Best Model Accuracy: {best_acc}")
print(f"Best Hyper parameters: Learning_rate {best_lr} and Betas {best_betas}")

"""## Save best Model"""

# save model
from pathlib import Path

MODEL_PATH = Path("drive/MyDrive/Course/Sem2/DLOps")
MODEL_PATH.mkdir(parents = True, exist_ok = True)

MODEL_NAME = "best_model.pth"
MODEL_PATH_SAVE = MODEL_PATH / MODEL_NAME

print(f"model saved at: {MODEL_PATH_SAVE}")
torch.save(obj = best_model.state_dict(), f = MODEL_PATH_SAVE)

"""## Task 3

## Confusion Matrix
"""

gc.collect()
torch.cuda.empty_cache()
# epoches = 15
lr = best_lr
betas = best_betas

rnn_train_loss = []
rnn_train_acc = []


torch.manual_seed(64)
torch.cuda.manual_seed(64)
n_hidden = 128
model_rnn = RNN(n_letters, n_hidden, n_categories).to(device)


nb_classes = 18

confusion_matrix = torch.zeros(nb_classes, nb_classes)
# SGD with momentum
# optimizer = torch.optim.SGD(params = resnet18.parameters(), lr = 0.1, momentum = 0.3)

# Adam as optimizer
optimizer = torch.optim.Adam(params = model_rnn.parameters(), lr = lr, 
                             betas = betas)

current_loss = 0
current_acc = 0
print_every = 1000
torch.manual_seed(64)
torch.cuda.manual_seed(64)
for iter in tqdm(range(1, len(val_set))):

    val_loss, val_acc, output = validation_loop(model = model_rnn, dataset = val_set[iter],
                                                loss_fn = loss_fn, optimizer = optimizer,
                                                lr = lr, accuracy_fn = accuracy_fn, device = device)
    
    # print()
    current_loss += val_loss
    current_acc += val_acc


    # Add current loss avg to list of losses
    if iter % (print_every-1) == 9:
        loss = current_loss / print_every
        acc = current_acc / print_every
        rnn_val_loss.append(loss)
        rnn_val_acc.append(acc)
        # print(train_loss)
        # print(train_acc)
        print(f"Epoch: {iter+1}  Train Loss: {train_loss:.4f} | Train Accuray: {acc:.4f}")
        # print(current_loss, current_acc)
        current_loss = 0
        current_acc = 0

    input, classes = val_set[iter]
    _, preds = torch.max(output, 1)
    for t, p in zip(classes.view(-1), preds.view(-1)):
            confusion_matrix[t.long(), p.long()] += 1

print(confusion_matrix)

"""## Task 4

## Add one linear layer and perform task 2 and 3 again
"""

class RNNPlusOne(nn.Module):
    def __init__(self, input_size, hidden_size, hidden_2, output_size):
        super().__init__()

        self.hidden_size = hidden_size
        # self.hidden_2 = hidden_2

        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        self.main_layer = nn.Sequential(
                                  nn.Linear(input_size + hidden_size, hidden_2),
                                  nn.Linear(hidden_2, output_size),
                                  nn.LogSoftmax(dim=1)
                                )

    def forward(self, input, hidden):
        combined = torch.cat((input, hidden), 1)
        hidden = self.i2h(combined)
        output = self.main_layer(combined)
        # output = self.softmax(output)
        return output, hidden

    def init_Hidden(self):
        return torch.zeros(1, self.hidden_size)

n_hidden = 128
hidden_2 = 64
model_rnn_one = RNNPlusOne(n_letters, n_hidden, hidden_2, n_categories).to(device)
model_rnn_one

"""## train model"""

gc.collect()
torch.cuda.empty_cache()
# epoches = 15
lr = 0.001

rnn_train_loss = []
rnn_train_acc = []


torch.manual_seed(64)
torch.cuda.manual_seed(64)
n_hidden = 128
hidden_2 = 64
model_rnn_one = RNNPlusOne(n_letters, n_hidden, hidden_2, n_categories).to(device)


# SGD with momentum
# optimizer = torch.optim.SGD(params = resnet18.parameters(), lr = 0.1, momentum = 0.3)

# Adam as optimizer
optimizer = torch.optim.Adam(params = model_rnn_one.parameters(), lr = lr, 
                             betas = (0.9, 0.99))

current_loss = 0
current_acc = 0
print_every = 5000
torch.manual_seed(64)
torch.cuda.manual_seed(64)
for iter in tqdm(range(1, len(train_set))):



    train_loss, train_acc = train_loop(model = model_rnn_one, dataset = train_set[iter],
                                       loss_fn = loss_fn, optimizer = optimizer,
                                       lr = lr, accuracy_fn = accuracy_fn, device = device)
    
    # print()
    current_loss += train_loss
    current_acc += train_acc


    # Add current loss avg to list of losses
    if iter % print_every == 0:
        loss = current_loss / print_every
        acc = current_acc / print_every
        rnn_train_loss.append(loss)
        rnn_train_acc.append(acc)
        # print(train_loss)
        # print(train_acc)
        print(f"Epoch: {iter}  Train Loss: {train_loss:.4f} | Train Accuray: {acc:.4f}")
        # print(current_loss, current_acc)
        current_loss = 0
        current_acc = 0
        # break

plot_graph(rnn_train_loss, rnn_train_acc)

"""## Hyper parameter tuning for plus one layer"""

print_every = 1000


# Hyperparameter
lr_rate = [0.001, 0.003, 0.005, 0.007, 0.0001] # learning rate
betas=[(0.8, 0.88),(0.9, 0.99)] # coefficients used for computing running averages of gradient and its square

best_model = RNNPlusOne(n_letters, n_hidden, hidden_2, n_categories).to(device)
best_model_losses, best_model_acces = [], []
best_acc = 0
best_lr = None
best_betas = None

hyper_params = [(lr,b) for lr in lr_rate for b in betas]
# print(parms_combs)

cur_iter,total_iter = 1, len(lr_rate)*len(betas)

for h_p in hyper_params:
  rnn_val_loss = []
  rnn_val_acc = []
  acc = 0
  print()
  print(f"current exp | total: {cur_iter} | {total_iter}")
  print(f"Training with --- lr rate: {h_p[0]}, betas: {h_p[1]}")
  n_hidden = 128
  hidden_2 = 64
  model_rnn_one = RNNPlusOne(n_letters, n_hidden, hidden_2, n_categories).to(device)

  optimizer = torch.optim.Adam(params=model_rnn_one.parameters(), lr=h_p[0], betas=h_p[1])

  torch.manual_seed(64)
  torch.cuda.manual_seed(64)
  for iter in tqdm(range(1, len(val_set))):

    val_loss, val_acc, output = validation_loop(model = model_rnn_one, dataset = val_set[iter],
                                         loss_fn = loss_fn, optimizer = optimizer,
                                         lr = lr, accuracy_fn = accuracy_fn, device = device)
    
    # print()
    current_loss += val_loss
    current_acc += val_acc


    # Add current loss avg to list of losses
    if iter % (print_every-1) == 9:
        loss = current_loss / print_every
        acc = current_acc / print_every
        rnn_val_loss.append(loss)
        rnn_val_acc.append(acc)
        # print(train_loss)
        # print(train_acc)
        print(f"Epoch: {iter+1}  Train Loss: {train_loss:.4f} | Train Accuray: {acc:.4f}")
        # print(current_loss, current_acc)
        current_loss = 0
        current_acc = 0
    
  if best_acc < acc:
    best_acc = acc
    best_lr = h_p[0]
    best_betas = h_p[1]
    best_model_losses = rnn_val_loss.copy()
    best_model_acces = rnn_val_acc.copy()
    best_model.load_state_dict(model_rnn_one.state_dict())

  cur_iter += 1

"""## Best model Loss and Accuray plot"""

plot_graph(best_model_losses, best_model_acces)

print(f"Best Model Accuracy: {best_acc}")
print(f"Best Hyper parameters: Learning_rate {best_lr} and Betas {best_betas}")

"""## Save best Model"""

# save model
from pathlib import Path

MODEL_PATH = Path("drive/MyDrive/Course/Sem2/DLOps")
MODEL_PATH.mkdir(parents = True, exist_ok = True)

MODEL_NAME = "best_model_one.pth"
MODEL_PATH_SAVE = MODEL_PATH / MODEL_NAME

print(f"model saved at: {MODEL_PATH_SAVE}")
torch.save(obj = best_model.state_dict(), f = MODEL_PATH_SAVE)

"""## Confusion Matrix"""

gc.collect()
torch.cuda.empty_cache()
# epoches = 15
lr = best_lr
betas = best_betas

rnn_train_loss = []
rnn_train_acc = []


torch.manual_seed(64)
torch.cuda.manual_seed(64)
n_hidden = 128
hidden_2 = 64
model_rnn_one = RNNPlusOne(n_letters, n_hidden, hidden_2, n_categories).to(device)


nb_classes = 18

confusion_matrix = torch.zeros(nb_classes, nb_classes)
# SGD with momentum
# optimizer = torch.optim.SGD(params = resnet18.parameters(), lr = 0.1, momentum = 0.3)

# Adam as optimizer
optimizer = torch.optim.Adam(params = model_rnn_one.parameters(), lr = lr, 
                             betas = betas)

current_loss = 0
current_acc = 0
print_every = 1000
torch.manual_seed(64)
torch.cuda.manual_seed(64)
for iter in tqdm(range(1, len(val_set))):

    val_loss, val_acc, output = validation_loop(model = model_rnn_one, dataset = val_set[iter],
                                                loss_fn = loss_fn, optimizer = optimizer,
                                                lr = lr, accuracy_fn = accuracy_fn, device = device)
    
    # print()
    current_loss += val_loss
    current_acc += val_acc


    # Add current loss avg to list of losses
    if iter % (print_every-1) == 9:
        loss = current_loss / print_every
        acc = current_acc / print_every
        rnn_val_loss.append(loss)
        rnn_val_acc.append(acc)
        # print(train_loss)
        # print(train_acc)
        print(f"Epoch: {iter+1}  Train Loss: {train_loss:.4f} | Train Accuray: {acc:.4f}")
        # print(current_loss, current_acc)
        current_loss = 0
        current_acc = 0

    input, classes = val_set[iter]
    _, preds = torch.max(output, 1)
    for t, p in zip(classes.view(-1), preds.view(-1)):
            confusion_matrix[t.long(), p.long()] += 1

print(confusion_matrix)

"""## Task 5

## Test on model_rnn
"""

gc.collect()
torch.cuda.empty_cache()
# epoches = 15

rnn_test_loss = []
rnn_test_acc = []


torch.manual_seed(64)
torch.cuda.manual_seed(64)
# n_hidden = 128
# model_rnn = RNN(n_letters, n_hidden, n_categories).to(device)

# SGD with momentum
# optimizer = torch.optim.SGD(params = resnet18.parameters(), lr = 0.1, momentum = 0.3)


current_loss = 0
current_acc = 0
print_every = 1000
torch.manual_seed(64)
torch.cuda.manual_seed(64)
for iter in tqdm(range(1, len(test_set))):



    test_loss, test_acc = test_loop(model = model_rnn, dataset = test_set[iter],
                                      loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                      lr = lr, device = device)
    
    # print()
    current_loss += test_loss
    current_acc += test_acc


    # Add current loss avg to list of losses
    if iter % print_every == 0:
        loss = current_loss / print_every
        acc = current_acc / print_every
        rnn_test_loss.append(loss)
        rnn_test_acc.append(acc)
        # print(train_loss)
        # print(train_acc)
        print(f"Epoch: {iter}  Train Loss: {test_loss:.4f} | Train Accuray: {acc:.4f}")
        # print(current_loss, current_acc)
        current_loss = 0
        current_acc = 0

plot_graph(rnn_test_loss, rnn_test_acc)

"""## Test model on model_rnn_one"""

gc.collect()
torch.cuda.empty_cache()
# epoches = 15

rnn_test_loss = []
rnn_test_acc = []


torch.manual_seed(64)
torch.cuda.manual_seed(64)
# n_hidden = 128
# model_rnn = RNN(n_letters, n_hidden, n_categories).to(device)

# SGD with momentum
# optimizer = torch.optim.SGD(params = resnet18.parameters(), lr = 0.1, momentum = 0.3)


current_loss = 0
current_acc = 0
print_every = 1000
torch.manual_seed(64)
torch.cuda.manual_seed(64)
for iter in tqdm(range(1, len(test_set))):



    test_loss, test_acc = test_loop(model = model_rnn_one, dataset = test_set[iter],
                                      loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                      lr = lr, device = device)
    
    # print()
    current_loss += test_loss
    current_acc += test_acc


    # Add current loss avg to list of losses
    if iter % print_every == 0:
        loss = current_loss / print_every
        acc = current_acc / print_every
        rnn_test_loss.append(loss)
        rnn_test_acc.append(acc)
        # print(train_loss)
        # print(train_acc)
        print(f"Epoch: {iter}  Train Loss: {test_loss:.4f} | Train Accuray: {acc:.4f}")
        # print(current_loss, current_acc)
        current_loss = 0
        current_acc = 0

plot_graph(rnn_test_loss, rnn_test_acc)

"""## Task 6

## Build stacked RNN
"""

class StackRNN(nn.Module):
    def __init__(self, num_letters, hidden_size, output_size, num_layers):
        super().__init__()

        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.gru = nn.GRU(
            input_size = num_letters, 
            hidden_size = hidden_size, 
            num_layers = num_layers,
        )
        self.fc = nn.Linear(hidden_size, output_size)


    def forward(self, input):
        hidden_state = self.init_hidden()
        output, hidden_state = self.gru(input, hidden_state)
        output = self.fc(output[-1])
        return output

    def init_hidden(self):
        return torch.zeros(1, self.hidden_size)

n_hidden = 128
model_stack_rnn = StackRNN(n_letters, n_hidden, n_categories, num_layers = 2).to(device)
model_stack_rnn

"""## train-test loop for stack model"""

# train
def stack_train_loop(model: torch.nn.Module, dataset: torch.utils.data.Dataset,
               loss_fn : torch.nn.Module, optimizer: torch.optim.Optimizer,
               lr: float, accuracy_fn, device: torch.device = device):
  
  train_loss, train_acc = 0, 0 
  # hidden1 = model.init_Hidden()
  # hidden2 = model.init_Hidden()


  # model.zero_grad()
  x_train, y_train = dataset

    
  if device == 'cuda':
    x_train, y_train = x_train.to(device), y_train.to(device)

  
  model.train()

  # 1. Forward
  for i in range(x_train.size()[0]):
    pred = model(x_train[i, i+1])
    

  # 2. Loss
  loss = loss_fn(pred, y_train)


  # Add parameters' gradients to their values, multiplied by learning rate
  # for para in model.parameters():
  #     para.data.add_(para.grad.data, alpha=-lr)

  # 3. Grad zerostep
  optimizer.zero_grad()

  # 4. Backward
  loss.backward()

  # 5. Optimizer step
  optimizer.step()

  # print(torch.argmax(pred, dim=1))
  
  # print(y_train)

  # print(torch.argmax(pred, dim=0))
  acc = (y_train == torch.argmax(pred, dim=1)).to(device)
  train_loss += loss
  train_acc += acc

  # print(train_loss, train_acc)

  return train_loss.item(), train_acc.item()


# validation_set
def stack_validation_loop(model: torch.nn.Module, dataset: torch.utils.data.Dataset,
                    loss_fn : torch.nn.Module, optimizer: torch.optim.Optimizer,
                    lr: float, accuracy_fn, device: torch.device = device):
  
  val_loss, val_acc = 0, 0 

  # model.zero_grad()

  # hidden1 = model.init_Hidden()
  # hidden2 = model.init_Hidden()

  x_val, y_val = dataset

    
  if device == 'cuda':
    x_val, y_val = x_val.to(device), y_val.to(device)

  
  model.train()

  # 1. Forward
  # for i in range(x_val.size()[0]):
  pred = model(x_val)

  # 2. Loss
  loss = loss_fn(pred, y_val)

  
  # # 3. Grad zerostep
  optimizer.zero_grad()

  # 4. Backward
  loss.backward()

  # Add parameters' gradients to their values, multiplied by learning rate
  # for para in model.parameters():
  #     para.data.add_(para.grad.data, alpha=-lr)

  # 5. Optimizer step
  optimizer.step()

  # print(torch.argmax(pred, dim=1))
  
  # print(y_train)

  # print(torch.argmax(pred, dim=0))
  acc = (y_val == torch.argmax(pred, dim=1)).to(device)
  val_loss += loss
  val_acc += acc

  return val_loss.item(), val_acc.item(), pred


# test
def stack_test_loop(model: torch.nn.Module, dataset: torch.utils.data.Dataset,
              loss_fn: torch.nn.Module, accuracy_fn, 
              lr: float, device: torch.device = device):
  
  test_loss, test_acc = 0, 0
  # hidden1 = model.init_Hidden()
  # hidden2 = model.init_Hidden()
  model.eval()
  with torch.inference_mode():
    x_test, y_test = dataset

    if device == 'cuda':
      x_test, y_test = x_test.to(device), y_test.to(device)

    # 1. Forward
    # for i in range(x_test.size()[0]):
    test_pred = model(x_test)
    
    # 2. Loss and accuray
    test_loss += loss_fn(test_pred, y_test)
    test_acc += (y_test == torch.argmax(test_pred, dim=1)).to(device)

  return test_loss.item(), test_acc.item()

"""## train model"""

gc.collect()
torch.cuda.empty_cache()
# epoches = 15
lr = 0.001

rnn_train_loss = []
rnn_train_acc = []


torch.manual_seed(64)
torch.cuda.manual_seed(64)
n_hidden = 128
model_stack_rnn = StackRNN(n_letters, n_hidden, n_categories, num_layers = 2).to(device)

# SGD with momentum
# optimizer = torch.optim.SGD(params = resnet18.parameters(), lr = 0.1, momentum = 0.3)

# Adam as optimizer
optimizer = torch.optim.Adam(params = model_stack_rnn.parameters(), lr = lr, 
                             betas = (0.9, 0.99))

current_loss = 0
current_acc = 0
print_every = 5000
torch.manual_seed(64)
torch.cuda.manual_seed(64)
for iter in tqdm(range(1, len(train_set))):



    train_loss, train_acc = stack_train_loop(model = model_stack_rnn, dataset = train_set[iter],
                                       loss_fn = loss_fn, optimizer = optimizer,
                                       lr = lr, accuracy_fn = accuracy_fn, device = device)
    
    # print()
    current_loss += train_loss
    current_acc += train_acc


    # Add current loss avg to list of losses
    if iter % print_every == 0:
        loss = current_loss / print_every
        acc = current_acc / print_every
        rnn_train_loss.append(loss)
        rnn_train_acc.append(acc)
        # print(train_loss)
        # print(train_acc)
        print(f"Epoch: {iter}  Train Loss: {train_loss:.4f} | Train Accuray: {acc:.4f}")
        # print(current_loss, current_acc)
        current_loss = 0
        current_acc = 0
        # break

"""## inference on word"""

def evaluate(line_tensor):
    hidden = model_rnn.init_Hidden()
    hidden = hidden.to(device)
    line_tensor =line_tensor.to(device)
    for i in range(line_tensor.size()[0]):
        output, hidden = model_rnn(line_tensor[i], hidden)

    return output

def predict(input_line, n_predictions=3):
    print('\n> %s' % input_line)
    with torch.no_grad():
        output = evaluate(lineToTensor(input_line))

        # Get top N categories
        topv, topi = output.topk(n_predictions, 1, True)
        predictions = []

        for i in range(n_predictions):
            value = topv[0][i].item()
            category_index = topi[0][i].item()
            print('(%.2f) %s' % (value, all_categories[category_index]))
            predictions.append([value, all_categories[category_index]])

predict('Emilia')
predict('Alexandra')
predict('Sachiko')
predict('Vladimir')
predict('Minh')
predict('Xi')
predict('Muammar')
predict('Mukesh')
predict('Andrew')
predict('Ronaldo')

