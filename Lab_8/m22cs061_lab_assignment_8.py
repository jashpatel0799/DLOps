# -*- coding: utf-8 -*-
"""M22CS061_Lab_Assignment_8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mJGQyoG69h5uk7tADupYzVVgYuhlQ8uQ
"""

!pip install wandb -q

"""## Import Require Libraries"""

import torch
from torch import nn

import torchvision
from torchvision import models, transforms
from torchvision import datasets
from torchvision import transforms
from torch.utils.data import DataLoader

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

from tqdm.auto import tqdm
from timeit import default_timer as timer

!pip install -q torchmetrics
from torchmetrics.classification import MulticlassAccuracy

import wandb

API_KEY = '881252af31786a1cf813449b9b4124955f54703e'

wandb.login(key=API_KEY)
print("[LOG]: Login Succesfull.")

"""## Device agnostic code"""

# device agnostic code
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

"""## Load Data"""

transform = transforms.Compose([
                                transforms.Resize((32, 32)),
                                transforms.ToTensor()
                              ])

aug_trasform = transforms.Compose([
                                   transforms.Resize((32, 32)),
                                   transforms.RandomRotation(15),
                                   transforms.RandomCrop(10),
                                   transforms.ToTensor()
                                  ])

train_dataset = datasets.FashionMNIST(root = '/data', train = True, transform = transform, download = True)
aug_train_dataset = datasets.FashionMNIST(root = '/data', train = True, transform = aug_trasform, download = True)
test_dataset = datasets.FashionMNIST(root = '/data', train = False, transform = transform, download = True)

"""## get only odd classes"""

train_list = []
train_aug_list = []
test_list = []


for i, (x,y) in enumerate(train_dataset):
  if y % 2 != 0:
    train_list.append(train_dataset[i])
  # print(train_data[i])
  # break

# for i, (x,y) in enumerate(aug_train_dataset):
#   if y % 2 != 0:
#     train_aug_list.append(train_dataset[i])

for x,y in test_dataset:
  if y % 2 != 0:
    test_list.append((x, y))

len(train_list), len(test_list), len(train_aug_list)

# concate datasets
# new_train_data = torch.utils.data.ConcatDataset((train_list, train_aug_list))
new_train_data = torch.utils.data.ConcatDataset([train_list])
new_test_data = torch.utils.data.ConcatDataset([test_list])

len(new_train_data), len(new_test_data)

"""## Make dataloader"""

BATCH_SIZE = 32
train_dataloader = DataLoader(dataset = new_train_data, batch_size = BATCH_SIZE, shuffle = True)
test_dataloader = DataLoader(dataset = new_test_data, batch_size = BATCH_SIZE, shuffle = False)

print(f'Total {len(train_dataloader)} of train each of {BATCH_SIZE} batches.')
print(f'Total {len(test_dataloader)} of test each of {BATCH_SIZE} batches.')

# class name
class_names = [train_dataset.classes[i] for i in range(len(train_dataset.classes)) if i % 2 != 0] 
class_names

index_cls = train_dataset.class_to_idx
index_cls

for i, (x,y) in enumerate(train_dataloader):
  print(y)
  break

image, label = new_train_data[0]
print(f"Image: {image.shape}")
plt.imshow(image.permute(1,2,0))
plt.title(class_names[label//2])
# image

"""## Train and test loop"""

# train
def train_loop(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer,
               accuracy_fn, device: torch.device = device):
  
  train_loss, train_acc = 0, 0

  model.train()

  for batch, (x_train, y_train) in enumerate(dataloader):

    Y = []
    for i in y_train:
      if i.item() % 2 == 0:
        Y.append((i.item()//2)+1)
      else:
        Y.append(i.item()//2)
    
    y_train = torch.tensor(Y)
    if device == 'cuda':
      x_train, y_train = x_train.to(device), y_train.to(device)
    

    # 1. Forward step
    pred = model(x_train)

    # 2. Loss
    loss = loss_fn(pred, y_train)

    # 3. Grad zerostep
    optimizer.zero_grad()

    # 4. Backward
    loss.backward()

    # 5. Optimizer step
    optimizer.step()

    # print(torch.argmax(pred, dim=1))
    
    # print(y_train)

    # print(torch.argmax(pred, dim=0))
    acc = accuracy_fn(y_train, torch.argmax(pred, dim=1))
    train_loss += loss
    train_acc += acc

  train_loss /= len(dataloader)
  train_acc /= len(dataloader)

  return train_loss, train_acc


# test
def test_loop(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module, accuracy_fn, 
              device: torch.device = device):
  
  test_loss, test_acc = 0, 0
  model.eval()
  with torch.inference_mode():
    for x_test, y_test in dataloader:

      Y = []
      for i in y_test:
        if i.item() % 2 == 0:
          Y.append((i.item()//2)+1)
        else:
          Y.append(i.item()//2)
      
      y_test = torch.tensor(Y)
      
      if device == 'cuda':
        x_test, y_test = x_test.to(device), y_test.to(device)

      # 1. Forward
      test_pred = model(x_test)
      
      # 2. Loss and accuray
      test_loss += loss_fn(test_pred, y_test)
      test_acc += accuracy_fn(y_test, torch.argmax(test_pred, dim=1))


    test_loss /= len(dataloader)
    test_acc /= len(dataloader)

  return test_loss, test_acc

"""## Plot Function"""

def plot_graph(train_losses, test_losses, train_accs, test_accs):
  plt.figure(figsize = (20, 8))
  plt.subplot(1, 2, 1)
  plt.plot(range(len(train_losses)), train_losses, label = "Train Loss")
  plt.plot(range(len(test_losses)), test_losses, label = "Test Loss")
  plt.legend()
  plt.xlabel("Epoches")
  plt.ylabel("Loss")
  # plt.show()

  plt.subplot(1, 2, 2)
  plt.plot(range(len(train_accs)), train_accs, label = "Train Accuracy")
  plt.plot(range(len(test_accs)), test_accs, label = "Test Accuracy")
  plt.legend()
  plt.xlabel("Epoches")
  plt.ylabel("Accuracy")
  plt.show()

"""## Loss and Accuracy Function"""

loss_fn = nn.CrossEntropyLoss()

accuracy_fn = MulticlassAccuracy(num_classes = len(class_names)).to(device)

"""## set activation function"""

# def funct(list_mods):
#   # print("type: ", type(list_mods))
#   for i in range(len(list_mods)):
#     if list_mods[i].__class__.__name__ == "ReLU":
#       list_mods[i] = nn.Tanh()
#     elif list_mods[i].__class__.__name__ in ("Sequential", "BasicBlock"):
#       list_mods[i] = nn.Sequential(*funct(list(list_mods[i].children())))
#   return list_mods

"""## Load model"""

model_18 = models.resnet18(weights = models.ResNet18_Weights.IMAGENET1K_V1, progress = False).to(device)
model_18.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)
feature_number = model_18.fc.in_features
model_18.fc = nn.Linear(feature_number, len(class_names)).to(device)

model_18

activation_function = ['Tanh', 'RELU']
# optimizer_1 = ("Adam", torch.optim.Adam(params = model_18.parameters(), lr = 1e-3))
# optimizer_2 = ("SGD", torch.optim.SGD(params = model_18.parameters(), lr = 1e-2))

optimizer = ["Adam", "SGD"]

# resnet18_selu = nn.Sequential(*funct(list(model_18.children()))).to(device)
# resnet18_selu

"""## Hyper Parameters"""

hyper_para = [(activation, opti) for activation in activation_function for opti in optimizer]

"""## Helper function"""

def validate_model(model, valid_dl, loss_fn, accuracy_fn, log_images=False, batch_idx=0, device: torch.device = device):
    "Compute performance of the model on the validation dataset and log a wandb.Table"
    model.eval()
    val_loss, val_acc = 0, 0.
    with torch.inference_mode():
        correct = 0
        for i, (images, labels) in enumerate(valid_dl):
            images, labels = images.to(device), labels.to(device)



            Y = []
            for i in labels:
              if i.item() % 2 == 0:
                Y.append((i.item()//2)+1)
              else:
                Y.append(i.item()//2)
            
            labels = torch.tensor(Y)

            if device == 'cuda':
              images, labels = images.to(device), labels.to(device)

            # Forward pass ‚û°
            outputs = model(images)
            # val_loss += loss_func(outputs, labels)*labels.size(0)

            # Compute accuracy and accumulate
            val_loss += loss_fn(outputs, labels)
            val_acc += accuracy_fn(labels, torch.argmax(outputs, dim=1))

            # Log one batch of images to the dashboard, always same batch_idx.
            if i==batch_idx and log_images:
                log_image_table(images, torch.max(outputs.data, 1), labels, outputs.softmax(dim=1))

        val_loss /= len(test_dataloader)
        val_acc /= len(test_dataloader)
    return val_loss, val_acc

def log_image_table(images, predicted, labels, probs):
    "Log a wandb.Table with (img, pred, target, scores)"
    # üêù Create a wandb Table to log images, labels and predictions to
    table = wandb.Table(columns=["image", "pred", "target"]+[f"score_{i}" for i in range(10)])
    for img, pred, targ, prob in zip(images.to("cpu"), predicted.to("cpu"), labels.to("cpu"), probs.to("cpu")):
        table.add_data(wandb.Image(img[0].numpy()*255), pred, targ, *prob.numpy())
    wandb.log({"predictions_table":table}, commit=False)

"""## train model with wandb"""

# init. epochs
epoches = 30

current, total_hy = 1, len(activation_function) * len(optimizer)


for h_p in hyper_para:
  print(f'Current iter: {current}/{total_hy}')
  print(f'Activation function: {h_p[0]} and Optimizer: {h_p[1]}')
  print()

  start_time = timer()

  model_18 = models.resnet18(weights = 'DEFAULT', progress = False)
  model_18.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  feature_number = model_18.fc.in_features
  model_18.fc = nn.Linear(feature_number, len(class_names))
  model_18 = model_18.to(device)

  if h_p[0] == 'Tanh':
    # model_18 = nn.Sequential(*funct(list(model_18.children()))).to(device)
    for name,child in model_18.named_children():
      if isinstance(child,nn.ReLU) or isinstance(child,nn.Tanh):
          model_18._modules['relu'] = nn.Tanh()


  if h_p[1] == 'Adam':
    optim = torch.optim.Adam(params = model_18.parameters(), lr = 1e-3)
  else:
    optim = torch.optim.SGD(params = model_18.parameters(), lr = 1e-2)

  wandb.init(
        # set the wandb project where this run will be logged
        project="dlops-lab-assi-8",
        
        # track hyperparameters and run metadata
        config={
        "activation_function": h_p[0],
        "optimizer": h_p[1],
        "architecture": "Resnet18",
        "dataset": "FashionMNIST",
        "epochs": epoches,
        }
    )

  
  model_18_train_loss, model_18_test_loss = [], []
  model_18_train_accs, model_18_test_accs = [], []
  print()

  torch.manual_seed(64)
  torch.cuda.manual_seed(64)
  for epoch in tqdm(range(epoches)):
    print(f"Epoch: {epoch+1}")
    train_loss, train_acc = train_loop(model = model_18, dataloader = train_dataloader,
                                      loss_fn = loss_fn, optimizer = optim,
                                      accuracy_fn = accuracy_fn, device = device)
    
    test_loss, test_acc = test_loop(model = model_18, dataloader = test_dataloader,
                                    loss_fn = loss_fn, accuracy_fn = accuracy_fn,
                                    device = device)
    
    val_loss, accuracy = validate_model(model_18, test_dataloader, loss_fn, accuracy_fn, log_images=(epoch==(wandb.config.epochs-1)), device = device)
    
    model_18_train_loss.append(train_loss.item())
    model_18_test_loss.append(test_loss.item())
    model_18_train_accs.append(train_acc.item())
    model_18_test_accs.append(test_acc.item())


    print(f"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Train Accuray: {train_acc:.4f} | Test Accuracy: {test_acc:.4f}")
    print()

  wandb.log({"Train Loss": train_loss, "Train Accuracy": train_acc, "Validation Loss": val_loss, "Validation Accuracy": accuracy})

  plot_graph(model_18_train_loss, model_18_test_loss, model_18_train_accs, model_18_test_accs)

  end_time = timer()

  print(f"Execution time of {current} iter: {end_time - start_time} Seconds.")

  wandb.finish()

  current += 1
  # save_model('cifair100_model_18.pth', model_18)
  # print("Model saved")